{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5d6b13d-10a1-45be-a470-81a3e8a0f6ee"
    }
   },
   "source": [
    "# Introducción al Aprendizaje por Refuerzos\n",
    "\n",
    "* Introducción. Modelo Agente Entorno. Agente Situado. Arquitectura Actor-Crítico.\n",
    "* Aprendizaje por Refuerzos. Elementos. Ciclo del Aprendizaje por Refuerzos. Definición Formal.\n",
    "* Procesos de Decisión de Markov. Función de Valor. Ecuación de Bellman. Optimalidad.\n",
    "* Aproximaciones al Aprendizaje. Model Free y Model Based.\n",
    "    * Iteración de Política.\n",
    "    * Iteración de Valor.\n",
    "* Ejercicios.\n",
    "\n",
    "## 5to año - Ingeniería en Sistemas de Información\n",
    "\n",
    "### Facultad Regional Villa María"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción: Entidad Inteligente -> Agente Situado\n",
    "* El desarrollo de la inteligencia requiere que la entidad o el agente esté situada/o en un entorno **(Measuring universal intelligence: Towards an anytime intelligence test, Hernandez-Orallo & Dowe, Artificial Intelligence, 2010).**\n",
    "![](images/Situated Agent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent-Environment Framework\n",
    "* El agente y su entorno interactúan a través de la ejecución de acciones, observación de estados y señales rewards. La inteligencia tendrá efecto sólo si el agente tiene claramente definidos objetivos o metas que persigue activamente mientras ocurre la interacción.\n",
    "![](images/AE Interaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura Actor-Crítico\n",
    "![](images/RL Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por Refuerzos\n",
    "\n",
    "* La toma de decisiones secuencial involucra aprender sobre nuestro entorno y elegir acciones que maximizan el retorno esperado. El RL computacional, inspirado por estas ideas, las formalizo y produjo un impacto importante en robótica, machine learning y neurociencias.\n",
    "\n",
    "* El Aprendizaje por Refuerzos (RL) consiste en un agente que se encuentra en algún estado $s \\in S$ inmerso en un entorno $E$ y toma acciones $a \\in A$ en busca de una meta. El agente puede ser modelado formalmente como una función f, que toma un  historial de interacción como entrada, y devuelve una acción a tomar. Una manera conveniente para representar el agente es una medida de probabilidad sobre el set A de acciones, en base a un historial de interacción: $$ f(a_{n}|s_{1}a_{1} r_{2} s_{2}a_{2}...r_{n}s_{n}) $$ que representa la probabilidad de la acción a en el ciclo n dado un historial de interacción.\n",
    "\n",
    "* Problema RL: ¿Cómo el agente produce la distribución de probabilidad sobre las acciones?\n",
    "\n",
    "* Dilema de exploración - explotación: debido a que el Agente no recibe ejemplos de entrenamiento, debe probar alternativas, procesar los resultados de sus acciones y modificar su comportamiento en algún sentido. ¿Cuándo explotar este conocimiento vs. cuándo probar nuevas estrategias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos del Aprendizaje por Refuerzos\n",
    "![](images/RL Elements.png)\n",
    "\n",
    "* **Policy (Política): **\n",
    "\n",
    "Una política define la manera de comportarse de un agente, en cualquier momento de tiempo dado. Basicamente, es un mapeo de un estado o percepción s a una acción a, pudiendo ser estocásticas.\n",
    "\n",
    "* **Reward Function (Función de Recompensa)**\n",
    "\n",
    "Define cuantitativamente el objetivo del agente. Es un mapeo de un par estado-acción a un número real que indica \"cuán deseable\" es ejecutar dicha acción en ese estado. Asimismo, el único objetivo del agente es maximizar la recompensa total que recibe a lo large del tiempo. Cabe mencionar que, si bien la función de reward no puede ser alterada por el agente, provee las bases para cambiar la política del mismo.\n",
    "\n",
    "* **Value function (Función de Valor)**\n",
    "\n",
    "La función de valor se diferencia de la función de reward en el sentido de que indica \"cuán deseable\" es, a largo plazo, ejecutar una acción en un determinado estado. Así, el valor de un estado s es la cantidad total de reward que el agente espera obtener a futuro comenzando la interacción en el estado s.\n",
    "\n",
    "* **Environment (Entorno)**\n",
    "\n",
    "El entorno se encuentra constitutido por todo aquel elemento (real o simulado) que el agente no puede controlar. Es con quién el agente interactúa a partir de la ejecución de acciones de control.\n",
    "\n",
    "## Ciclo del Aprendizaje por Refuerzos\n",
    "![](images/RL Cycle.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "84a1b72e-5d2e-4343-a014-0fcfee9373e4"
    }
   },
   "source": [
    "### Definición formal\n",
    "\n",
    "* Si el problema de RL dado tiene un conjunto finito de estados y acciones y satisface la propiedad de Markov entonces puede definirse como un Proceso de Decisión de Markov\n",
    "\n",
    "\\begin{equation}\n",
    "MDPFinito = (S, A, P(.), R(.), γ)\n",
    "\\end{equation}\n",
    "\n",
    "donde\n",
    "\n",
    "$$ S = {s_{1}, s_{2}, ..., s_{n}} $$\n",
    "es un conjunto finito de estados.\n",
    "$$ A = {a_{1}, a_{2}, ..., a_{m}} $$\n",
    "es un conjunto finito de acciones.\n",
    "$$ P_{a}(s,s') = P(s_{t+1} = s | s_{t} = s, a_{t} = a) $$ \n",
    "es la probabilidad de que la acción a tomada en tiempo t y en estado s lleve al agente al estado s' en tiempo t+1\n",
    "$$ R_{a}(s,s') $$\n",
    "es la recompensa inmediata recibido tras transicionar, luego de tomar la acción a, desde el estado s al estado s'\n",
    "$$\\gamma \\in  [0,1]$$ \n",
    "es el factor de descuento, representando la diferencia en la importancia de la recompensa a corto plazo vs la recompensa a largo plazo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mdp_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "197f95d9-38cd-4f22-830a-1c484037b895"
    }
   },
   "source": [
    "* Un episodio (instancia) de este MDP forma una secuencia finita \n",
    "$$s_{0}, a_{0}, r_{1}, s_{1}, a_{1}, r_{2}, s_{2}, ... , s_{n-1}, a_{n-1}, r_{n}, s_{n} $$ \n",
    "donde $$s_{n}$$\n",
    "es un estado final (o n es el tiempo de corte).\n",
    "\n",
    "* La recompensa total del episodio está dado por \n",
    "$$ R = r_{1} + r_{2} + ... + r_{n} $$\n",
    "\n",
    "* En consecuencia, la recompensa a futuro partiendo del tiempo $t$ está dado por \n",
    "$$R_{t} = r_{t} + r_{t+1} + ... $$\n",
    "\n",
    "* Hay que considerar que el ambiente es estocástico en la mayor parte de los entornos reales y, por tanto, la recompensa suele diverger mientras más alejado se encuentre el instante de tiempo considerado. Es por esto que se utiliza un parámetro $γ$ llamado _factor de descuento_, para descontar el valor de las recompensas futuras. De esta manera,\n",
    "\n",
    "\\begin{equation} R_{t} = r_{t} + γr_{t+1} + γ^2r_{t+2} + γ^3r_{t+3} + ... = r_{t} + γ(r_{t+1} + γr_{t+2} + γ^2r_{t+3} ...) = r_{t} + γR_{t+1} \\end{equation}\n",
    "\n",
    "* Si utilizamos $γ=0$, el agente priorizará sólo la recompensa inmediata, mientras que $\\gamma=1$ hará que considere todas los recompensas de la misma manera, independientemente del momento en donde las reciba.\n",
    "![](images/RL Problem Statement.png)\n",
    "![](images/Policy Definition.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesos de Decisión de Markov\n",
    "\n",
    "### Función de Valor\n",
    "\n",
    "* El valor de un estado es el retorno esperado por el agente, comenzando la interacción en dicho estado, dependiendo de la política ejecutada por el agente.\n",
    "\n",
    "![](images/Funcion de Estado Valor.png)\n",
    "\n",
    "* El valor de la ejecución de una acción en un estado es el retorno esperado por el agente, comenzando la interacción en dicho estado a partir de la ejecución de dicha acción, dependiendo de la política ejecutada por el agente.\n",
    "\n",
    "![](images/Funcion de Accion Valor.png)\n",
    "\n",
    "Una propiedad fundamental de las funciones de valor es que satisfacen ciertas propiedades recursivas. Para cualquier política  π y cualquier estado s, V(s) y Q(s,a) pueden ser definidas recursivamente en términos de la denominada *Ecuación de Bellman* ** (Bellman, 1957) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuación de Bellman\n",
    "\n",
    "* La idea básica es:\n",
    "\n",
    "![](images/Retorno.png)\n",
    "\n",
    "* Entonces,\n",
    "\n",
    "![](images/Retorno - Valor.png)\n",
    "\n",
    "* O, sin el operador de valor esperado:\n",
    "\n",
    "![](images/Bellman Equation.png)\n",
    "\n",
    "La ecuación anterior refleja el hecho de que el valor de un estado se encuentra definido en términos de la recompensa inmediata y los valores de los estados siguientes ponderados en función de las probabilidades de transición, y adicionalmente un factor de descuento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuación de Optimalidad de Bellman\n",
    "\n",
    "La Ecuación de Optimalidad de Bellman refleja el hecho de que el Valor de un estado bajo la política óptima debe ser igual al retorno esperado para la mejor acción en dicho estado:\n",
    "\n",
    "![](images/Ecuacion de Optimalidad Valor.png)\n",
    "\n",
    "Al mismo tiempo, la acción óptima para un estado s dada la función de valor, puede obtenerse mediante:\n",
    "\n",
    "![](images/Accion Optima.png)\n",
    "\n",
    "La política anterior se denomina **Política Greedy**, dado que selecciona la mejor acción para cada estado, teniendo en cuenta la función de valor V(s). De manera análoga, la función de acción-valor óptima puede expresarse como:\n",
    "\n",
    "![](images/Accion Valor Optima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aproximaciones para el aprendizaje de V y Q\n",
    "\n",
    "![](images/Aproximaciones al aprendizaje.png)\n",
    "\n",
    "### Model Based vs. Model Free\n",
    "\n",
    "* Model-free aprende Q/V directamente y presenta muy baja complejidad computacional.\n",
    "\n",
    "* Model-based aprende T y R y usa un algoritmo de planning para encontrar la política. Uso eficiente de los datos/experiencia. Alto costo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programación Dinámica: Iteración de Valor e Iteración de Política (Model Based)\n",
    "\n",
    "#### Iteración de Valor\n",
    "\n",
    "![](images/Iteracion de Valor.png)\n",
    "\n",
    "#### Iteración de Política\n",
    "\n",
    "![](images/Iteracion de Politica.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Fecha de entrega: **14/06/2017**\n",
    "\n",
    "Nota: la resolución de los ejercicios es **individual**; en el caso de que dos ejercicios enviados contengan un código igual o muy similar (sin considerar los comentarios), se los considerará a ambos como desaprobados. La reutilización del código de los notebooks está permitida (por ejemplo para confeccionar gráficos).\n",
    "\n",
    "1. Un entorno denominado **\"gridworld\"** consiste en un agente que se mueve en una grilla formada por un conjunto de celdas, cada una de las cuales se corresponde con un estado. En cada una de las celdas, el agente puede ejecutar una entre cuatro acciones posibles: norte, sur, este y oeste, las que producen el efecto de mover el agente hacia la celda adyacente de acuerdo a la acción ejecutada (de manera determinística). Aquella acción que lleva al agente fuera de la grilla, tiene el efecto de mantener al mismo en la misma celda, pero producen una recompensa de -1. Las demás acciones producen una recompensa de 0, excepto aquellas que mueven al agente fuera de los estados especiales denominados A y B. Desde el estado A, las cuatro acciones producen una recompensa de 10, y el efecto es que el esado siguiente siempre es A'. Lo mismo ocurre con el estado B, excepto que la recompensa es 5 y el estado siguiente es B' (Ver figura inferior a)).\n",
    "\n",
    "![](images/Gridworld.png)\n",
    "\n",
    "La parte b) de la figura, muestra la función de valor calculada para un agente que actúa de manera aleatoria, es decir, aquel que siempre elige las acciones de manera equiprobable, obtenidas a partir de la aplicación de la siguiente implementación de Iteración de Valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política Aleatoria\n",
      "[[ 3.30902999  8.78932551  4.42765281  5.3224012   1.49221235]\n",
      " [ 1.52162172  2.9923515   2.25017358  1.90760531  0.5474363 ]\n",
      " [ 0.05085614  0.73820423  0.67314689  0.35821982 -0.40310755]\n",
      " [-0.97355865 -0.43546179 -0.35484864 -0.58557148 -1.18304148]\n",
      " [-1.8576669  -1.34519762 -1.22923364 -1.42288454 -1.97514545]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from python_utils.import_ import import_global\n",
    "import numpy as np\n",
    "\n",
    "# Límites de la grilla y posiciones especiales\n",
    "WORLD_SIZE = 5\n",
    "A_POS = [0, 1]\n",
    "A_PRIME_POS = [4, 1]\n",
    "B_POS = [0, 3]\n",
    "B_PRIME_POS = [2, 3]\n",
    "discount = 0.9\n",
    "\n",
    "world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "\n",
    "# acciones left, up, right, down\n",
    "actions = ['L', 'U', 'R', 'D']\n",
    "\n",
    "# se agrega en actionProb la probabilidad de las acciones para la política\n",
    "actionProb = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    actionProb.append([])\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        actionProb[i].append(dict({'L':0.25, 'U':0.25, 'R':0.25, 'D':0.25}))\n",
    "\n",
    "# se setea la función de transición y la función de reward        \n",
    "nextState = []\n",
    "actionReward = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    nextState.append([])\n",
    "    actionReward.append([])\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        next = dict()\n",
    "        reward = dict()\n",
    "        if i == 0:\n",
    "            next['U'] = [i, j]\n",
    "            reward['U'] = -1.0\n",
    "        else:\n",
    "            next['U'] = [i - 1, j]\n",
    "            reward['U'] = 0.0\n",
    "\n",
    "        if i == WORLD_SIZE - 1:\n",
    "            next['D'] = [i, j]\n",
    "            reward['D'] = -1.0\n",
    "        else:\n",
    "            next['D'] = [i + 1, j]\n",
    "            reward['D'] = 0.0\n",
    "\n",
    "        if j == 0:\n",
    "            next['L'] = [i, j]\n",
    "            reward['L'] = -1.0\n",
    "        else:\n",
    "            next['L'] = [i, j - 1]\n",
    "            reward['L'] = 0.0\n",
    "\n",
    "        if j == WORLD_SIZE - 1:\n",
    "            next['R'] = [i, j]\n",
    "            reward['R'] = -1.0\n",
    "        else:\n",
    "            next['R'] = [i, j + 1]\n",
    "            reward['R'] = 0.0\n",
    "\n",
    "        if [i, j] == A_POS:\n",
    "            next['L'] = next['R'] = next['D'] = next['U'] = A_PRIME_POS\n",
    "            reward['L'] = reward['R'] = reward['D'] = reward['U'] = 10.0\n",
    "\n",
    "        if [i, j] == B_POS:\n",
    "            next['L'] = next['R'] = next['D'] = next['U'] = B_PRIME_POS\n",
    "            reward['L'] = reward['R'] = reward['D'] = reward['U'] = 5.0\n",
    "\n",
    "        nextState[i].append(next)\n",
    "        actionReward[i].append(reward)\n",
    "\n",
    "#Iteración de Valor\n",
    "while True:\n",
    "    # Se itera hasta lograr la convergencia\n",
    "    newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    for i in range(0, WORLD_SIZE):\n",
    "        for j in range(0, WORLD_SIZE):\n",
    "            for action in actions:\n",
    "                newPosition = nextState[i][j][action]\n",
    "                # Actualización basada en Bellman\n",
    "                newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]])\n",
    "    if np.sum(np.abs(world - newWorld)) < 1e-4:\n",
    "        print('Política Aleatoria')\n",
    "        print(newWorld)\n",
    "        break\n",
    "    world = newWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Genere una gráfica que muestre la evolución del cálculo \"np.sum(np.abs(world - newWorld))\", para cada paso de actualización realizado hasta lograr la convergencia.\n",
    "\n",
    "1.2 Modifique el algoritmo anterior para encontrar el valor de la política óptima. Genere una gráfica que muestre la evolución del cálculo \"np.sum(np.abs(world - newWorld))\", para cada paso de actualización realizado hasta lograr la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Un robot de reciclaje de residuos debe decidir, en cada instante de tiempo, si busca activamente un contenedor de residuos, si permanece en el lugar en que se encuentra a la espera de que alguien le traiga un contenedor de residuos, o bien si debe volver a su base para recargar la batería. La mejor forma de encontrar contenedores es buscarlos, pero dicha acción reduce la carga de la batería, mientras que la acción de esperar no. Por otra parte, en cualquier caso en que el robot se encuentre buscando contenedores, existe la posibilidad de que la batería se agote. En este caso, el robot se apaga y necesita ser rescatado (produciendo una recompensa muy baja). Asuma que el problema puede ser modelado de la manera que se muestra en la figura siguiente (Diagrama y función de transición de estados), en donde $R_{search}$ es el número esperado de contenedores que se espera encontrar mientras se ejecuta search, y $R_{wait}$ es el número esperado de contenedores que se espera recibir mientras se ejecuta wait, y $R_{search} > R_{wait}$.\n",
    "\n",
    "![](images/Recycling Robot.png)\n",
    "\n",
    "2.1 Implemente un algoritmo de Iteración de Valor para obtener la política óptima del robot de reciclaje.\n",
    "\n",
    "2.2 Utilice el algoritmo implementado en (2.1) para evaluar cómo cambia el valor de la política óptima a partir de alterar  $\\alpha$, $\\beta$ para un valor de $R_{search}=5 $ y $R_{wait} = 2$. Para dicha evaluación, emplee una gráfica que permita determinar cuáles son los valores de dichas variables que maximizan el retorno esperado por el agente en cada estado. AYUDA: Varíe algoritmicamente los valores de $\\alpha$ y $\\beta$ y calcule la política óptima correspondiente. La gráfica debería presentar tres ejes: $\\alpha$, $\\beta$, y una variable que totalice los valores de los estados.\n",
    "\n",
    "2.3 Con los mejores valores de $\\alpha$ y $\\beta$ obtenidos en 2.2, realice la misma operación variando $R_{search} $  con un tope de 10, manteniendo $R_{wait}$ = en 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Un agente debe aprender a llegar en la menor cantidad de pasos desde la posición S a la posición G en una grilla como la que sigue.\n",
    "\n",
    "![](images/Gridworld Labyrinth.png)\n",
    "\n",
    "Las acciones disponibles en cada estado son las mismas que las descriptas para el agente del Ejercicio 1. El efecto de una acción que llevaría al agente fuera de la grilla, es que el agente vuelve al estado S.\n",
    "\n",
    "3.1 Plantee una función de recompensa que permita al agente aprender a lograr el objetivo expresado en 3.\n",
    "\n",
    "3.2 Implemente un algoritmo basado en Iteración de Valor para aprender la política óptima en el entorno especificado.\n",
    "\n",
    "3.3 Realice una gráfica que permita evaluar como cambia el valor de la política óptima en relación al factor de descuento $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Implemente la solución de los ejercicios anteriores empleando Iteración de Política. Puede utilizar como base de implementación el siguiente código fuente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política Inicial\n",
      "[0, 0, 0, 0, 0]\n",
      "State 1 : q_sa 85.0 q_best 0.0\n",
      "State 3 : q_sa 1000.75 q_best 0.0\n",
      "State 4 : q_sa 1.75 q_best 1.0\n",
      "Iteracion: 1\n",
      "State 0 : q_sa 64.75 q_best 1.0\n",
      "State 2 : q_sa 850.5625 q_best 100.0\n",
      "State 3 : q_sa 1001.3125 q_best 1000.75\n",
      "State 4 : q_sa 2.3125 q_best 1.75\n",
      "Iteracion: 2\n",
      "State 1 : q_sa 647.921875 q_best 85.0\n",
      "State 2 : q_sa 850.984375 q_best 850.5625\n",
      "State 3 : q_sa 1001.734375 q_best 1001.3125\n",
      "State 4 : q_sa 2.734375 q_best 2.3125\n",
      "Iteracion: 3\n",
      "State 0 : q_sa 486.94140625 q_best 64.75\n",
      "State 1 : q_sa 648.23828125 q_best 647.921875\n",
      "State 2 : q_sa 851.30078125 q_best 850.984375\n",
      "State 3 : q_sa 1002.05078125 q_best 1001.734375\n",
      "State 4 : q_sa 3.05078125 q_best 2.734375\n",
      "Iteracion: 4\n",
      "State 0 : q_sa 487.178710938 q_best 486.94140625\n",
      "State 1 : q_sa 648.475585938 q_best 648.23828125\n",
      "State 2 : q_sa 851.538085938 q_best 851.30078125\n",
      "State 3 : q_sa 1002.28808594 q_best 1002.05078125\n",
      "State 4 : q_sa 3.2880859375 q_best 3.05078125\n",
      "Iteracion: 5\n",
      "State 0 : q_sa 487.356689453 q_best 487.178710938\n",
      "State 1 : q_sa 648.653564453 q_best 648.475585938\n",
      "State 2 : q_sa 851.716064453 q_best 851.538085938\n",
      "State 3 : q_sa 1002.46606445 q_best 1002.28808594\n",
      "State 4 : q_sa 3.46606445313 q_best 3.2880859375\n",
      "Iteracion: 6\n",
      "State 0 : q_sa 487.49017334 q_best 487.356689453\n",
      "State 1 : q_sa 648.78704834 q_best 648.653564453\n",
      "State 2 : q_sa 851.84954834 q_best 851.716064453\n",
      "State 3 : q_sa 1002.59954834 q_best 1002.46606445\n",
      "State 4 : q_sa 3.59954833984 q_best 3.46606445313\n",
      "Iteracion: 7\n",
      "State 0 : q_sa 487.590286255 q_best 487.49017334\n",
      "State 1 : q_sa 648.887161255 q_best 648.78704834\n",
      "State 2 : q_sa 851.949661255 q_best 851.84954834\n",
      "State 3 : q_sa 1002.69966125 q_best 1002.59954834\n",
      "State 4 : q_sa 3.69966125488 q_best 3.59954833984\n",
      "Iteracion: 8\n",
      "State 0 : q_sa 487.665370941 q_best 487.590286255\n",
      "State 1 : q_sa 648.962245941 q_best 648.887161255\n",
      "State 2 : q_sa 852.024745941 q_best 851.949661255\n",
      "State 3 : q_sa 1002.77474594 q_best 1002.69966125\n",
      "State 4 : q_sa 3.77474594116 q_best 3.69966125488\n",
      "Iteracion: 9\n",
      "State 0 : q_sa 487.721684456 q_best 487.665370941\n",
      "State 1 : q_sa 649.018559456 q_best 648.962245941\n",
      "State 2 : q_sa 852.081059456 q_best 852.024745941\n",
      "State 3 : q_sa 1002.83105946 q_best 1002.77474594\n",
      "State 4 : q_sa 3.83105945587 q_best 3.77474594116\n",
      "Iteracion: 10\n",
      "State 0 : q_sa 487.763919592 q_best 487.721684456\n",
      "State 1 : q_sa 649.060794592 q_best 649.018559456\n",
      "State 2 : q_sa 852.123294592 q_best 852.081059456\n",
      "State 3 : q_sa 1002.87329459 q_best 1002.83105946\n",
      "State 4 : q_sa 3.8732945919 q_best 3.83105945587\n",
      "Iteracion: 11\n",
      "State 0 : q_sa 487.795595944 q_best 487.763919592\n",
      "State 1 : q_sa 649.092470944 q_best 649.060794592\n",
      "State 2 : q_sa 852.154970944 q_best 852.123294592\n",
      "State 3 : q_sa 1002.90497094 q_best 1002.87329459\n",
      "State 4 : q_sa 3.90497094393 q_best 3.8732945919\n",
      "Iteracion: 12\n",
      "State 0 : q_sa 487.819353208 q_best 487.795595944\n",
      "State 1 : q_sa 649.116228208 q_best 649.092470944\n",
      "State 2 : q_sa 852.178728208 q_best 852.154970944\n",
      "State 3 : q_sa 1002.92872821 q_best 1002.90497094\n",
      "State 4 : q_sa 3.92872820795 q_best 3.90497094393\n",
      "Iteracion: 13\n",
      "State 0 : q_sa 487.837171156 q_best 487.819353208\n",
      "State 1 : q_sa 649.134046156 q_best 649.116228208\n",
      "State 2 : q_sa 852.196546156 q_best 852.178728208\n",
      "State 3 : q_sa 1002.94654616 q_best 1002.92872821\n",
      "State 4 : q_sa 3.94654615596 q_best 3.92872820795\n",
      "Iteracion: 14\n",
      "State 0 : q_sa 487.850534617 q_best 487.837171156\n",
      "State 1 : q_sa 649.147409617 q_best 649.134046156\n",
      "State 2 : q_sa 852.209909617 q_best 852.196546156\n",
      "State 3 : q_sa 1002.95990962 q_best 1002.94654616\n",
      "State 4 : q_sa 3.95990961697 q_best 3.94654615596\n",
      "Iteracion: 15\n",
      "State 0 : q_sa 487.860557213 q_best 487.850534617\n",
      "State 1 : q_sa 649.157432213 q_best 649.147409617\n",
      "State 2 : q_sa 852.219932213 q_best 852.209909617\n",
      "State 3 : q_sa 1002.96993221 q_best 1002.95990962\n",
      "State 4 : q_sa 3.96993221273 q_best 3.95990961697\n",
      "Iteracion: 16\n",
      "State 0 : q_sa 487.86807416 q_best 487.860557213\n",
      "State 1 : q_sa 649.16494916 q_best 649.157432213\n",
      "State 2 : q_sa 852.22744916 q_best 852.219932213\n",
      "State 3 : q_sa 1002.97744916 q_best 1002.96993221\n",
      "State 4 : q_sa 3.97744915955 q_best 3.96993221273\n",
      "Iteracion: 17\n",
      "State 0 : q_sa 487.87371187 q_best 487.86807416\n",
      "State 1 : q_sa 649.17058687 q_best 649.16494916\n",
      "State 2 : q_sa 852.23308687 q_best 852.22744916\n",
      "State 3 : q_sa 1002.98308687 q_best 1002.97744916\n",
      "State 4 : q_sa 3.98308686966 q_best 3.97744915955\n",
      "Iteracion: 18\n",
      "State 0 : q_sa 487.877940152 q_best 487.87371187\n",
      "State 1 : q_sa 649.174815152 q_best 649.17058687\n",
      "State 2 : q_sa 852.237315152 q_best 852.23308687\n",
      "State 3 : q_sa 1002.98731515 q_best 1002.98308687\n",
      "State 4 : q_sa 3.98731515224 q_best 3.98308686966\n",
      "Iteracion: 19\n",
      "State 0 : q_sa 487.881111364 q_best 487.877940152\n",
      "State 1 : q_sa 649.177986364 q_best 649.174815152\n",
      "State 2 : q_sa 852.240486364 q_best 852.237315152\n",
      "State 3 : q_sa 1002.99048636 q_best 1002.98731515\n",
      "State 4 : q_sa 3.99048636418 q_best 3.98731515224\n",
      "Iteracion: 20\n",
      "State 0 : q_sa 487.883489773 q_best 487.881111364\n",
      "State 1 : q_sa 649.180364773 q_best 649.177986364\n",
      "State 2 : q_sa 852.242864773 q_best 852.240486364\n",
      "State 3 : q_sa 1002.99286477 q_best 1002.99048636\n",
      "State 4 : q_sa 3.99286477314 q_best 3.99048636418\n",
      "Iteracion: 21\n",
      "State 0 : q_sa 487.88527358 q_best 487.883489773\n",
      "State 1 : q_sa 649.18214858 q_best 649.180364773\n",
      "State 2 : q_sa 852.24464858 q_best 852.242864773\n",
      "State 3 : q_sa 1002.99464858 q_best 1002.99286477\n",
      "State 4 : q_sa 3.99464857985 q_best 3.99286477314\n",
      "Iteracion: 22\n",
      "State 0 : q_sa 487.886611435 q_best 487.88527358\n",
      "State 1 : q_sa 649.183486435 q_best 649.18214858\n",
      "State 2 : q_sa 852.245986435 q_best 852.24464858\n",
      "State 3 : q_sa 1002.99598643 q_best 1002.99464858\n",
      "State 4 : q_sa 3.99598643489 q_best 3.99464857985\n",
      "Iteracion: 23\n",
      "State 0 : q_sa 487.887614826 q_best 487.886611435\n",
      "State 1 : q_sa 649.184489826 q_best 649.183486435\n",
      "State 2 : q_sa 852.246989826 q_best 852.245986435\n",
      "State 3 : q_sa 1002.99698983 q_best 1002.99598643\n",
      "State 4 : q_sa 3.99698982617 q_best 3.99598643489\n",
      "Iteracion: 24\n",
      "State 0 : q_sa 487.88836737 q_best 487.887614826\n",
      "State 1 : q_sa 649.18524237 q_best 649.184489826\n",
      "State 2 : q_sa 852.24774237 q_best 852.246989826\n",
      "State 3 : q_sa 1002.99774237 q_best 1002.99698983\n",
      "State 4 : q_sa 3.99774236963 q_best 3.99698982617\n",
      "Iteracion: 25\n",
      "State 0 : q_sa 487.888931777 q_best 487.88836737\n",
      "State 1 : q_sa 649.185806777 q_best 649.18524237\n",
      "State 2 : q_sa 852.248306777 q_best 852.24774237\n",
      "State 3 : q_sa 1002.99830678 q_best 1002.99774237\n",
      "State 4 : q_sa 3.99830677722 q_best 3.99774236963\n",
      "Iteracion: 26\n",
      "State 0 : q_sa 487.889355083 q_best 487.888931777\n",
      "State 1 : q_sa 649.186230083 q_best 649.185806777\n",
      "State 2 : q_sa 852.248730083 q_best 852.248306777\n",
      "State 3 : q_sa 1002.99873008 q_best 1002.99830678\n",
      "State 4 : q_sa 3.99873008291 q_best 3.99830677722\n",
      "Iteracion: 27\n",
      "State 0 : q_sa 487.889672562 q_best 487.889355083\n",
      "State 1 : q_sa 649.186547562 q_best 649.186230083\n",
      "State 2 : q_sa 852.249047562 q_best 852.248730083\n",
      "State 3 : q_sa 1002.99904756 q_best 1002.99873008\n",
      "State 4 : q_sa 3.99904756219 q_best 3.99873008291\n",
      "Iteracion: 28\n",
      "State 0 : q_sa 487.889910672 q_best 487.889672562\n",
      "State 1 : q_sa 649.186785672 q_best 649.186547562\n",
      "State 2 : q_sa 852.249285672 q_best 852.249047562\n",
      "State 3 : q_sa 1002.99928567 q_best 1002.99904756\n",
      "State 4 : q_sa 3.99928567164 q_best 3.99904756219\n",
      "Iteracion: 29\n",
      "State 0 : q_sa 487.890089254 q_best 487.889910672\n",
      "State 1 : q_sa 649.186964254 q_best 649.186785672\n",
      "State 2 : q_sa 852.249464254 q_best 852.249285672\n",
      "State 3 : q_sa 1002.99946425 q_best 1002.99928567\n",
      "State 4 : q_sa 3.99946425373 q_best 3.99928567164\n",
      "Iteracion: 30\n",
      "State 0 : q_sa 487.89022319 q_best 487.890089254\n",
      "State 1 : q_sa 649.18709819 q_best 649.186964254\n",
      "State 2 : q_sa 852.24959819 q_best 852.249464254\n",
      "State 3 : q_sa 1002.99959819 q_best 1002.99946425\n",
      "State 4 : q_sa 3.9995981903 q_best 3.99946425373\n",
      "Iteracion: 31\n",
      "State 0 : q_sa 487.890323643 q_best 487.89022319\n",
      "State 1 : q_sa 649.187198643 q_best 649.18709819\n",
      "State 2 : q_sa 852.249698643 q_best 852.24959819\n",
      "State 3 : q_sa 1002.99969864 q_best 1002.99959819\n",
      "State 4 : q_sa 3.99969864272 q_best 3.9995981903\n",
      "Iteracion: 32\n",
      "State 0 : q_sa 487.890398982 q_best 487.890323643\n",
      "State 1 : q_sa 649.187273982 q_best 649.187198643\n",
      "State 2 : q_sa 852.249773982 q_best 852.249698643\n",
      "State 3 : q_sa 1002.99977398 q_best 1002.99969864\n",
      "State 4 : q_sa 3.99977398204 q_best 3.99969864272\n",
      "Iteracion: 33\n",
      "State 0 : q_sa 487.890455487 q_best 487.890398982\n",
      "State 1 : q_sa 649.187330487 q_best 649.187273982\n",
      "State 2 : q_sa 852.249830487 q_best 852.249773982\n",
      "State 3 : q_sa 1002.99983049 q_best 1002.99977398\n",
      "State 4 : q_sa 3.99983048653 q_best 3.99977398204\n",
      "Iteracion: 34\n",
      "State 0 : q_sa 487.890497865 q_best 487.890455487\n",
      "State 1 : q_sa 649.187372865 q_best 649.187330487\n",
      "State 2 : q_sa 852.249872865 q_best 852.249830487\n",
      "State 3 : q_sa 1002.99987286 q_best 1002.99983049\n",
      "State 4 : q_sa 3.9998728649 q_best 3.99983048653\n",
      "Iteracion: 35\n",
      "State 0 : q_sa 487.890529649 q_best 487.890497865\n",
      "State 1 : q_sa 649.187404649 q_best 649.187372865\n",
      "State 2 : q_sa 852.249904649 q_best 852.249872865\n",
      "State 3 : q_sa 1002.99990465 q_best 1002.99987286\n",
      "State 4 : q_sa 3.99990464867 q_best 3.9998728649\n",
      "Iteracion: 36\n",
      "State 0 : q_sa 487.890553487 q_best 487.890529649\n",
      "State 1 : q_sa 649.187428487 q_best 649.187404649\n",
      "State 2 : q_sa 852.249928487 q_best 852.249904649\n",
      "State 3 : q_sa 1002.99992849 q_best 1002.99990465\n",
      "State 4 : q_sa 3.99992848651 q_best 3.99990464867\n",
      "Iteracion: 37\n",
      "State 0 : q_sa 487.890571365 q_best 487.890553487\n",
      "State 1 : q_sa 649.187446365 q_best 649.187428487\n",
      "State 2 : q_sa 852.249946365 q_best 852.249928487\n",
      "State 3 : q_sa 1002.99994636 q_best 1002.99992849\n",
      "State 4 : q_sa 3.99994636488 q_best 3.99992848651\n",
      "Iteracion: 38\n",
      "State 0 : q_sa 487.890584774 q_best 487.890571365\n",
      "State 1 : q_sa 649.187459774 q_best 649.187446365\n",
      "State 2 : q_sa 852.249959774 q_best 852.249946365\n",
      "State 3 : q_sa 1002.99995977 q_best 1002.99994636\n",
      "State 4 : q_sa 3.99995977366 q_best 3.99994636488\n",
      "Iteracion: 39\n",
      "State 0 : q_sa 487.89059483 q_best 487.890584774\n",
      "State 1 : q_sa 649.18746983 q_best 649.187459774\n",
      "State 2 : q_sa 852.24996983 q_best 852.249959774\n",
      "State 3 : q_sa 1002.99996983 q_best 1002.99995977\n",
      "State 4 : q_sa 3.99996983024 q_best 3.99995977366\n",
      "Iteracion: 40\n",
      "State 0 : q_sa 487.890602373 q_best 487.89059483\n",
      "State 1 : q_sa 649.187477373 q_best 649.18746983\n",
      "State 2 : q_sa 852.249977373 q_best 852.24996983\n",
      "State 3 : q_sa 1002.99997737 q_best 1002.99996983\n",
      "State 4 : q_sa 3.99997737268 q_best 3.99996983024\n",
      "Iteracion: 41\n",
      "State 0 : q_sa 487.89060803 q_best 487.890602373\n",
      "State 1 : q_sa 649.18748303 q_best 649.187477373\n",
      "State 2 : q_sa 852.24998303 q_best 852.249977373\n",
      "State 3 : q_sa 1002.99998303 q_best 1002.99997737\n",
      "State 4 : q_sa 3.99998302951 q_best 3.99997737268\n",
      "Iteracion: 42\n",
      "State 0 : q_sa 487.890612272 q_best 487.89060803\n",
      "State 1 : q_sa 649.187487272 q_best 649.18748303\n",
      "State 2 : q_sa 852.249987272 q_best 852.24998303\n",
      "State 3 : q_sa 1002.99998727 q_best 1002.99998303\n",
      "State 4 : q_sa 3.99998727213 q_best 3.99998302951\n",
      "Iteracion: 43\n",
      "State 0 : q_sa 487.890615454 q_best 487.890612272\n",
      "State 1 : q_sa 649.187490454 q_best 649.187487272\n",
      "State 2 : q_sa 852.249990454 q_best 852.249987272\n",
      "State 3 : q_sa 1002.99999045 q_best 1002.99998727\n",
      "State 4 : q_sa 3.9999904541 q_best 3.99998727213\n",
      "Iteracion: 44\n",
      "State 0 : q_sa 487.890617841 q_best 487.890615454\n",
      "State 1 : q_sa 649.187492841 q_best 649.187490454\n",
      "State 2 : q_sa 852.249992841 q_best 852.249990454\n",
      "State 3 : q_sa 1002.99999284 q_best 1002.99999045\n",
      "State 4 : q_sa 3.99999284058 q_best 3.9999904541\n",
      "Iteracion: 45\n",
      "State 0 : q_sa 487.89061963 q_best 487.890617841\n",
      "State 1 : q_sa 649.18749463 q_best 649.187492841\n",
      "State 2 : q_sa 852.24999463 q_best 852.249992841\n",
      "State 3 : q_sa 1002.99999463 q_best 1002.99999284\n",
      "State 4 : q_sa 3.99999463043 q_best 3.99999284058\n",
      "Iteracion: 46\n",
      "State 0 : q_sa 487.890620973 q_best 487.89061963\n",
      "State 1 : q_sa 649.187495973 q_best 649.18749463\n",
      "State 2 : q_sa 852.249995973 q_best 852.24999463\n",
      "State 3 : q_sa 1002.99999597 q_best 1002.99999463\n",
      "State 4 : q_sa 3.99999597282 q_best 3.99999463043\n",
      "Iteracion: 47\n",
      "State 0 : q_sa 487.89062198 q_best 487.890620973\n",
      "State 1 : q_sa 649.18749698 q_best 649.187495973\n",
      "State 2 : q_sa 852.24999698 q_best 852.249995973\n",
      "State 3 : q_sa 1002.99999698 q_best 1002.99999597\n",
      "State 4 : q_sa 3.99999697962 q_best 3.99999597282\n",
      "Iteracion: 48\n",
      "State 0 : q_sa 487.890622735 q_best 487.89062198\n",
      "State 1 : q_sa 649.187497735 q_best 649.18749698\n",
      "State 2 : q_sa 852.249997735 q_best 852.24999698\n",
      "State 3 : q_sa 1002.99999773 q_best 1002.99999698\n",
      "State 4 : q_sa 3.99999773471 q_best 3.99999697962\n",
      "Iteracion: 49\n",
      "State 0 : q_sa 487.890623301 q_best 487.890622735\n",
      "State 1 : q_sa 649.187498301 q_best 649.187497735\n",
      "State 2 : q_sa 852.249998301 q_best 852.249997735\n",
      "State 3 : q_sa 1002.9999983 q_best 1002.99999773\n",
      "State 4 : q_sa 3.99999830104 q_best 3.99999773471\n",
      "Iteracion: 50\n",
      "State 0 : q_sa 487.890623726 q_best 487.890623301\n",
      "State 1 : q_sa 649.187498726 q_best 649.187498301\n",
      "State 2 : q_sa 852.249998726 q_best 852.249998301\n",
      "State 3 : q_sa 1002.99999873 q_best 1002.9999983\n",
      "State 4 : q_sa 3.99999872578 q_best 3.99999830104\n",
      "Iteracion: 51\n",
      "State 0 : q_sa 487.890624044 q_best 487.890623726\n",
      "State 1 : q_sa 649.187499044 q_best 649.187498726\n",
      "State 2 : q_sa 852.249999044 q_best 852.249998726\n",
      "State 3 : q_sa 1002.99999904 q_best 1002.99999873\n",
      "State 4 : q_sa 3.99999904433 q_best 3.99999872578\n",
      "Iteracion: 52\n",
      "State 0 : q_sa 487.890624283 q_best 487.890624044\n",
      "State 1 : q_sa 649.187499283 q_best 649.187499044\n",
      "State 2 : q_sa 852.249999283 q_best 852.249999044\n",
      "State 3 : q_sa 1002.99999928 q_best 1002.99999904\n",
      "State 4 : q_sa 3.99999928325 q_best 3.99999904433\n",
      "Iteracion: 53\n",
      "State 0 : q_sa 487.890624462 q_best 487.890624283\n",
      "State 1 : q_sa 649.187499462 q_best 649.187499283\n",
      "State 2 : q_sa 852.249999462 q_best 852.249999283\n",
      "State 3 : q_sa 1002.99999946 q_best 1002.99999928\n",
      "State 4 : q_sa 3.99999946244 q_best 3.99999928325\n",
      "Iteracion: 54\n",
      "State 0 : q_sa 487.890624597 q_best 487.890624462\n",
      "State 1 : q_sa 649.187499597 q_best 649.187499462\n",
      "State 2 : q_sa 852.249999597 q_best 852.249999462\n",
      "State 3 : q_sa 1002.9999996 q_best 1002.99999946\n",
      "State 4 : q_sa 3.99999959683 q_best 3.99999946244\n",
      "Iteracion: 55\n",
      "State 0 : q_sa 487.890624698 q_best 487.890624597\n",
      "State 1 : q_sa 649.187499698 q_best 649.187499597\n",
      "State 2 : q_sa 852.249999698 q_best 852.249999597\n",
      "State 3 : q_sa 1002.9999997 q_best 1002.9999996\n",
      "State 4 : q_sa 3.99999969762 q_best 3.99999959683\n",
      "Iteracion: 56\n",
      "State 0 : q_sa 487.890624773 q_best 487.890624698\n",
      "State 1 : q_sa 649.187499773 q_best 649.187499698\n",
      "State 2 : q_sa 852.249999773 q_best 852.249999698\n",
      "State 3 : q_sa 1002.99999977 q_best 1002.9999997\n",
      "State 4 : q_sa 3.99999977322 q_best 3.99999969762\n",
      "Iteracion: 57\n",
      "State 0 : q_sa 487.89062483 q_best 487.890624773\n",
      "State 1 : q_sa 649.18749983 q_best 649.187499773\n",
      "State 2 : q_sa 852.24999983 q_best 852.249999773\n",
      "State 3 : q_sa 1002.99999983 q_best 1002.99999977\n",
      "State 4 : q_sa 3.99999982991 q_best 3.99999977322\n",
      "Iteracion: 58\n",
      "State 0 : q_sa 487.890624872 q_best 487.89062483\n",
      "State 1 : q_sa 649.187499872 q_best 649.18749983\n",
      "State 2 : q_sa 852.249999872 q_best 852.24999983\n",
      "State 3 : q_sa 1002.99999987 q_best 1002.99999983\n",
      "State 4 : q_sa 3.99999987243 q_best 3.99999982991\n",
      "Iteracion: 59\n",
      "State 0 : q_sa 487.890624904 q_best 487.890624872\n",
      "State 1 : q_sa 649.187499904 q_best 649.187499872\n",
      "State 2 : q_sa 852.249999904 q_best 852.249999872\n",
      "State 3 : q_sa 1002.9999999 q_best 1002.99999987\n",
      "State 4 : q_sa 3.99999990433 q_best 3.99999987243\n",
      "Iteracion: 60\n",
      "State 0 : q_sa 487.890624928 q_best 487.890624904\n",
      "State 1 : q_sa 649.187499928 q_best 649.187499904\n",
      "State 2 : q_sa 852.249999928 q_best 852.249999904\n",
      "State 3 : q_sa 1002.99999993 q_best 1002.9999999\n",
      "State 4 : q_sa 3.99999992824 q_best 3.99999990433\n",
      "Iteracion: 61\n",
      "State 0 : q_sa 487.890624946 q_best 487.890624928\n",
      "State 1 : q_sa 649.187499946 q_best 649.187499928\n",
      "State 2 : q_sa 852.249999946 q_best 852.249999928\n",
      "State 3 : q_sa 1002.99999995 q_best 1002.99999993\n",
      "State 4 : q_sa 3.99999994618 q_best 3.99999992824\n",
      "Iteracion: 62\n",
      "State 0 : q_sa 487.89062496 q_best 487.890624946\n",
      "State 1 : q_sa 649.18749996 q_best 649.187499946\n",
      "State 2 : q_sa 852.24999996 q_best 852.249999946\n",
      "State 3 : q_sa 1002.99999996 q_best 1002.99999995\n",
      "State 4 : q_sa 3.99999995964 q_best 3.99999994618\n",
      "Iteracion: 63\n",
      "State 0 : q_sa 487.89062497 q_best 487.89062496\n",
      "State 1 : q_sa 649.18749997 q_best 649.18749996\n",
      "State 2 : q_sa 852.24999997 q_best 852.24999996\n",
      "State 3 : q_sa 1002.99999997 q_best 1002.99999996\n",
      "State 4 : q_sa 3.99999996973 q_best 3.99999995964\n",
      "Iteracion: 64\n",
      "State 0 : q_sa 487.890624977 q_best 487.89062497\n",
      "State 1 : q_sa 649.187499977 q_best 649.18749997\n",
      "State 2 : q_sa 852.249999977 q_best 852.24999997\n",
      "State 3 : q_sa 1002.99999998 q_best 1002.99999997\n",
      "State 4 : q_sa 3.9999999773 q_best 3.99999996973\n",
      "Iteracion: 65\n",
      "State 0 : q_sa 487.890624983 q_best 487.890624977\n",
      "State 1 : q_sa 649.187499983 q_best 649.187499977\n",
      "State 2 : q_sa 852.249999983 q_best 852.249999977\n",
      "State 3 : q_sa 1002.99999998 q_best 1002.99999998\n",
      "State 4 : q_sa 3.99999998297 q_best 3.9999999773\n",
      "Iteracion: 66\n",
      "State 0 : q_sa 487.890624987 q_best 487.890624983\n",
      "State 1 : q_sa 649.187499987 q_best 649.187499983\n",
      "State 2 : q_sa 852.249999987 q_best 852.249999983\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999998\n",
      "State 4 : q_sa 3.99999998723 q_best 3.99999998297\n",
      "Iteracion: 67\n",
      "State 0 : q_sa 487.89062499 q_best 487.890624987\n",
      "State 1 : q_sa 649.18749999 q_best 649.187499987\n",
      "State 2 : q_sa 852.24999999 q_best 852.249999987\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999042 q_best 3.99999998723\n",
      "Iteracion: 68\n",
      "State 0 : q_sa 487.890624993 q_best 487.89062499\n",
      "State 1 : q_sa 649.187499993 q_best 649.18749999\n",
      "State 2 : q_sa 852.249999993 q_best 852.24999999\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999282 q_best 3.99999999042\n",
      "Iteracion: 69\n",
      "State 0 : q_sa 487.890624995 q_best 487.890624993\n",
      "State 1 : q_sa 649.187499995 q_best 649.187499993\n",
      "State 2 : q_sa 852.249999995 q_best 852.249999993\n",
      "State 3 : q_sa 1002.99999999 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999461 q_best 3.99999999282\n",
      "Iteracion: 70\n",
      "State 0 : q_sa 487.890624996 q_best 487.890624995\n",
      "State 1 : q_sa 649.187499996 q_best 649.187499995\n",
      "State 2 : q_sa 852.249999996 q_best 852.249999995\n",
      "State 3 : q_sa 1003.0 q_best 1002.99999999\n",
      "State 4 : q_sa 3.99999999596 q_best 3.99999999461\n",
      "Iteracion: 71\n",
      "State 0 : q_sa 487.890624997 q_best 487.890624996\n",
      "State 1 : q_sa 649.187499997 q_best 649.187499996\n",
      "State 2 : q_sa 852.249999997 q_best 852.249999996\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999697 q_best 3.99999999596\n",
      "Iteracion: 72\n",
      "State 0 : q_sa 487.890624998 q_best 487.890624997\n",
      "State 1 : q_sa 649.187499998 q_best 649.187499997\n",
      "State 2 : q_sa 852.249999998 q_best 852.249999997\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999773 q_best 3.99999999697\n",
      "Iteracion: 73\n",
      "State 0 : q_sa 487.890624998 q_best 487.890624998\n",
      "State 1 : q_sa 649.187499998 q_best 649.187499998\n",
      "State 2 : q_sa 852.249999998 q_best 852.249999998\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999983 q_best 3.99999999773\n",
      "Iteracion: 74\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624998\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499998\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999998\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999872 q_best 3.9999999983\n",
      "Iteracion: 75\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624999\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499999\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999904 q_best 3.99999999872\n",
      "Iteracion: 76\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624999\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499999\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999928 q_best 3.99999999904\n",
      "Iteracion: 77\n",
      "State 0 : q_sa 487.890624999 q_best 487.890624999\n",
      "State 1 : q_sa 649.187499999 q_best 649.187499999\n",
      "State 2 : q_sa 852.249999999 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999946 q_best 3.99999999928\n",
      "Iteracion: 78\n",
      "State 0 : q_sa 487.890625 q_best 487.890624999\n",
      "State 1 : q_sa 649.1875 q_best 649.187499999\n",
      "State 2 : q_sa 852.25 q_best 852.249999999\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999996 q_best 3.99999999946\n",
      "Iteracion: 79\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999997 q_best 3.9999999996\n",
      "Iteracion: 80\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999977 q_best 3.9999999997\n",
      "Iteracion: 81\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999983 q_best 3.99999999977\n",
      "Iteracion: 82\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999987 q_best 3.99999999983\n",
      "Iteracion: 83\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.9999999999 q_best 3.99999999987\n",
      "Iteracion: 84\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999993 q_best 3.9999999999\n",
      "Iteracion: 85\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999995 q_best 3.99999999993\n",
      "Iteracion: 86\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999996 q_best 3.99999999995\n",
      "Iteracion: 87\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999997 q_best 3.99999999996\n",
      "Iteracion: 88\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999998 q_best 3.99999999997\n",
      "Iteracion: 89\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999998 q_best 3.99999999998\n",
      "Iteracion: 90\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999998\n",
      "Iteracion: 91\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999999\n",
      "Iteracion: 92\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999999\n",
      "Iteracion: 93\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 3.99999999999 q_best 3.99999999999\n",
      "Iteracion: 94\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 3.99999999999\n",
      "Iteracion: 95\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 96\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 97\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 98\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 99\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 100\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 101\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 102\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 103\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 104\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 105\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 106\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 107\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 108\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 109\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 3 : q_sa 1003.0 q_best 1003.0\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 110\n",
      "State 2 : q_sa 852.25 q_best 852.25\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 111\n",
      "State 1 : q_sa 649.1875 q_best 649.1875\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 112\n",
      "State 0 : q_sa 487.890625 q_best 487.890625\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 113\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 114\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 115\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 116\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 117\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 118\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 119\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 120\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 121\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 122\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 123\n",
      "State 4 : q_sa 4.0 q_best 4.0\n",
      "Iteracion: 124\n",
      "Iteracion: 125\n",
      "Política Final\n",
      "[0, 1, 0, 1, 0]\n",
      "[  487.890625   649.1875     852.25      1003.           4.      ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"Este es el pseudocódigo correspondiente al algoritmo implementado más abajo.\n",
    "\n",
    "1: Procedure Policy_Iteration(S,A,P,R)\n",
    "2:           Inputs\n",
    "3:                     S is the set of all states\n",
    "4:                     A is the set of all actions\n",
    "5:                     P is state transition function specifying P(s'|s,a)\n",
    "6:                     R is a reward function R(s,a,s')\n",
    "7:           Output\n",
    "8:                     optimal policy π\n",
    "9:           Local\n",
    "10:                     action array π[S]\n",
    "11:                     Boolean variable noChange\n",
    "12:                     real array V[S]\n",
    "13:           set π arbitrarily\n",
    "14:           repeat\n",
    "15:                     noChange ←true\n",
    "16:                     Solve V[s] = ∑s'∈S P(s'|s,π[s])(R(s,a,s')+γV[s'])\n",
    "17:                     for each s∈S do\n",
    "18:                               Let QBest=V[s]\n",
    "19:                               for each a ∈A do\n",
    "20:                                         Let Qsa=∑s'∈S P(s'|s,a)(R(s,a,s')+γV[s'])\n",
    "21:                                         if (Qsa > QBest) then\n",
    "22:                                                   π[s]←a\n",
    "23:                                                   QBest ←Qsa\n",
    "24:                                                   noChange ←false\n",
    "25:           until noChange\n",
    "26:           return π\n",
    "\"\"\"\n",
    "\n",
    "# aquí se setea un entorno de ejemplo\n",
    "states = [0,1,2,3,4]\n",
    "actions = [0,1]\n",
    "N_STATES = len(states)\n",
    "N_ACTIONS = len(actions)\n",
    "P = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # Probabilidades de Transición\n",
    "R = np.zeros((N_STATES, N_ACTIONS, N_STATES))  # Rewards\n",
    "\n",
    "P[0,0,1] = 1.0\n",
    "P[1,1,2] = 1.0\n",
    "P[2,0,3] = 1.0\n",
    "P[3,1,4] = 1.0\n",
    "P[4,0,4] = 1.0\n",
    "\n",
    "\n",
    "R[0,0,1] = 1\n",
    "R[1,1,2] = 10\n",
    "R[2,0,3] = 100\n",
    "R[3,1,4] = 1000\n",
    "R[4,0,4] = 1.0\n",
    "\n",
    "# factor de descuento\n",
    "gamma = 0.75\n",
    "\n",
    "# inicializa la política y la función de valor\n",
    "policy = [0 for s in range(N_STATES)]\n",
    "V = np.zeros(N_STATES)\n",
    "\n",
    "print(\"Política Inicial\")\n",
    "print(policy)\n",
    "\n",
    "is_value_changed = True\n",
    "iterations = 0\n",
    "while is_value_changed:\n",
    "    is_value_changed = False\n",
    "    iterations += 1\n",
    "    # corre la iteración de valor para cada estado \n",
    "    for s in range(N_STATES):\n",
    "        V[s] = sum([P[s,policy[s],s1] * (R[s,policy[s],s1] + gamma*V[s1]) for s1 in range(N_STATES)])\n",
    "        \n",
    "    # realiza la mejora de la política\n",
    "    for s in range(N_STATES):\n",
    "        q_best = V[s]\n",
    "        for a in range(N_ACTIONS):\n",
    "            q_sa = sum([P[s, a, s1] * (R[s, a, s1] + gamma * V[s1]) for s1 in range(N_STATES)])\n",
    "            if q_sa > q_best:\n",
    "                print(\"State\", s, \": q_sa\", q_sa, \"q_best\", q_best)\n",
    "                policy[s] = a\n",
    "                q_best = q_sa\n",
    "                is_value_changed = True\n",
    "\n",
    "    print (\"Iteracion:\", iterations)\n",
    "\n",
    "print (\"Política Final\")\n",
    "print (policy)\n",
    "print (V)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nbpresent": {
   "slides": {
    "0d01fc30-23a9-4dc7-a99a-309a23aa7a9b": {
     "id": "0d01fc30-23a9-4dc7-a99a-309a23aa7a9b",
     "prev": "899c419b-b69d-4a42-8c8f-ec9e170f2953",
     "regions": {
      "d7b7ed9a-ebe1-45b7-b520-ee8111741f6f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4951a012-93f0-4561-b2aa-f356d9fd10af",
        "part": "whole"
       },
       "id": "d7b7ed9a-ebe1-45b7-b520-ee8111741f6f"
      }
     }
    },
    "1265d14e-4373-4961-b094-fdc0f41bd665": {
     "id": "1265d14e-4373-4961-b094-fdc0f41bd665",
     "prev": "69ef8fab-1cf4-4791-8359-fbd8c36b87f5",
     "regions": {
      "60a1d8af-4d7e-49aa-923c-5bc0d4914c2c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c7d530e5-f080-4f70-9482-c4612911cbe3",
        "part": "whole"
       },
       "id": "60a1d8af-4d7e-49aa-923c-5bc0d4914c2c"
      }
     }
    },
    "1315de08-3be9-4fc0-81e0-acc069ac5044": {
     "id": "1315de08-3be9-4fc0-81e0-acc069ac5044",
     "prev": null,
     "regions": {
      "b47d0e6d-4dcb-4a20-8e5e-874847ad2b76": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d5d6b13d-10a1-45be-a470-81a3e8a0f6ee",
        "part": "whole"
       },
       "id": "b47d0e6d-4dcb-4a20-8e5e-874847ad2b76"
      }
     }
    },
    "1809123f-e0c6-4859-bc21-f3b5dc441a1b": {
     "id": "1809123f-e0c6-4859-bc21-f3b5dc441a1b",
     "prev": "3ad98c91-7a86-4a2a-9ecf-cfc6db0d3916",
     "regions": {
      "9c137fb8-1b4f-427e-9b34-49632cbebe6d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "037b358e-6d28-4df4-ae9d-974f8775e00e",
        "part": "whole"
       },
       "id": "9c137fb8-1b4f-427e-9b34-49632cbebe6d"
      }
     }
    },
    "1c00d850-e4dc-4916-bbe5-b275c71d185b": {
     "id": "1c00d850-e4dc-4916-bbe5-b275c71d185b",
     "prev": "982cfab9-3c43-45c9-9733-6e3c27538085",
     "regions": {
      "27107d62-642d-48d1-96c8-f9b6e21001d9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9da47f7b-28d6-44cb-bac2-daae14fab987",
        "part": "whole"
       },
       "id": "27107d62-642d-48d1-96c8-f9b6e21001d9"
      }
     }
    },
    "223fdf1f-941d-4667-819b-4082691bb812": {
     "id": "223fdf1f-941d-4667-819b-4082691bb812",
     "prev": "6afb007f-1120-46a8-8c15-2cdaa7286dab",
     "regions": {
      "3f59bb53-c37b-4f27-bc1e-1ab27bc75d99": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "58ec6acb-8f4f-4fda-af22-9f4703e72336",
        "part": "whole"
       },
       "id": "3f59bb53-c37b-4f27-bc1e-1ab27bc75d99"
      }
     }
    },
    "313101ba-a6ea-4985-b56a-7e5faeb4ef11": {
     "id": "313101ba-a6ea-4985-b56a-7e5faeb4ef11",
     "prev": "90bfd2ff-9a21-4de9-8584-dd17db513b9b",
     "regions": {
      "00901c26-d4cf-4a49-8a0e-4e61005f043c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2be788a6-2235-491e-aab5-0bb567fe525d",
        "part": "whole"
       },
       "id": "00901c26-d4cf-4a49-8a0e-4e61005f043c"
      }
     }
    },
    "3194ed44-3f5e-49b0-bdb0-38553756e922": {
     "id": "3194ed44-3f5e-49b0-bdb0-38553756e922",
     "prev": "f6cecdf6-a875-43e5-8544-93b4f9abb66d",
     "regions": {
      "541d2ce3-01f2-49f4-a6d1-f41765d0a18c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ef536408-da22-4ef2-808e-2a65e7081661",
        "part": "whole"
       },
       "id": "541d2ce3-01f2-49f4-a6d1-f41765d0a18c"
      }
     }
    },
    "3ad98c91-7a86-4a2a-9ecf-cfc6db0d3916": {
     "id": "3ad98c91-7a86-4a2a-9ecf-cfc6db0d3916",
     "prev": "d6d405a0-dfde-4a37-a754-7db5e780e0af",
     "regions": {
      "a200fdd8-1279-4bbb-9cba-eac12cf5dbcb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c4915b91-8222-4388-b0bc-3bb6a4d87b54",
        "part": "whole"
       },
       "id": "a200fdd8-1279-4bbb-9cba-eac12cf5dbcb"
      }
     }
    },
    "3ea64685-7182-46a8-b427-641f90be1dc9": {
     "id": "3ea64685-7182-46a8-b427-641f90be1dc9",
     "prev": "dcf0e1b4-2b46-4603-bb28-5f8e80ffd702",
     "regions": {
      "d14104e3-1e91-4507-be35-db71a6dd41a8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "eb46d21c-4caa-42a9-a197-4d64c603fd9b",
        "part": "whole"
       },
       "id": "d14104e3-1e91-4507-be35-db71a6dd41a8"
      }
     }
    },
    "4257ba12-3eec-48b3-8dc4-a92664887a5d": {
     "id": "4257ba12-3eec-48b3-8dc4-a92664887a5d",
     "prev": "92e07c6c-11e0-4795-bda1-abe9ddfd4d02",
     "regions": {
      "efaedb93-d56a-491b-8660-b8e2e5b07519": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9bc1fb76-8a78-4f38-a718-38d1e15cc1b3",
        "part": "whole"
       },
       "id": "efaedb93-d56a-491b-8660-b8e2e5b07519"
      }
     }
    },
    "4f4a9683-905f-4595-9760-d56a847ae47c": {
     "id": "4f4a9683-905f-4595-9760-d56a847ae47c",
     "prev": "4257ba12-3eec-48b3-8dc4-a92664887a5d",
     "regions": {
      "c00eb85e-4e9e-48ac-8119-2a14a98c2042": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6bee5ed5-2110-428b-9f5b-feb464858aaf",
        "part": "whole"
       },
       "id": "c00eb85e-4e9e-48ac-8119-2a14a98c2042"
      }
     }
    },
    "69ef8fab-1cf4-4791-8359-fbd8c36b87f5": {
     "id": "69ef8fab-1cf4-4791-8359-fbd8c36b87f5",
     "prev": "223fdf1f-941d-4667-819b-4082691bb812",
     "regions": {
      "58b36bb4-8aa5-44e0-b88d-db85a5ad1e5c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a61ee6dd-76b9-4ed4-93e7-e28a25ae4490",
        "part": "whole"
       },
       "id": "58b36bb4-8aa5-44e0-b88d-db85a5ad1e5c"
      }
     }
    },
    "6afb007f-1120-46a8-8c15-2cdaa7286dab": {
     "id": "6afb007f-1120-46a8-8c15-2cdaa7286dab",
     "prev": "dd511c07-9814-43e5-b7c0-31151b3582e9",
     "regions": {
      "d3c11af7-2f8c-4b6f-af02-8e5ab8a114db": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e118cd6c-e993-4a82-a052-caeb13fb8cad",
        "part": "whole"
       },
       "id": "d3c11af7-2f8c-4b6f-af02-8e5ab8a114db"
      }
     }
    },
    "6c974a58-be8c-42b2-a4e5-e31607f31727": {
     "id": "6c974a58-be8c-42b2-a4e5-e31607f31727",
     "prev": "1c00d850-e4dc-4916-bbe5-b275c71d185b",
     "regions": {
      "1b3a3c08-25e5-40a5-8faf-2855fea2e3ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f417f2ff-615d-4adc-bb45-0482f34e77c0",
        "part": "whole"
       },
       "id": "1b3a3c08-25e5-40a5-8faf-2855fea2e3ac"
      }
     }
    },
    "7d5b2ad9-38cb-4ce1-8c24-7774740b8710": {
     "id": "7d5b2ad9-38cb-4ce1-8c24-7774740b8710",
     "prev": "cef2e9fb-fb0c-4d2c-9264-fa08d3de7788",
     "regions": {
      "17c4f751-c034-4d71-9d8b-eaa8bad1602f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "197f95d9-38cd-4f22-830a-1c484037b895",
        "part": "whole"
       },
       "id": "17c4f751-c034-4d71-9d8b-eaa8bad1602f"
      }
     }
    },
    "899c419b-b69d-4a42-8c8f-ec9e170f2953": {
     "id": "899c419b-b69d-4a42-8c8f-ec9e170f2953",
     "prev": "6c974a58-be8c-42b2-a4e5-e31607f31727",
     "regions": {
      "0d0ff77c-47e7-4ecf-8542-348d395aa416": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "96467e64-3610-4e91-8034-b35e87ac68fb",
        "part": "whole"
       },
       "id": "0d0ff77c-47e7-4ecf-8542-348d395aa416"
      }
     }
    },
    "8ee0867a-0485-4903-9e16-0b1a83bad931": {
     "id": "8ee0867a-0485-4903-9e16-0b1a83bad931",
     "prev": "9b077531-fbbf-4d74-8bd4-55550025f933",
     "regions": {
      "26cefc3d-e27f-46ec-be1d-e8c0849eb199": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9248f5e9-f98c-458a-ae3a-71512b071a5b",
        "part": "whole"
       },
       "id": "26cefc3d-e27f-46ec-be1d-e8c0849eb199"
      }
     }
    },
    "90bfd2ff-9a21-4de9-8584-dd17db513b9b": {
     "id": "90bfd2ff-9a21-4de9-8584-dd17db513b9b",
     "prev": "adb231cc-0652-40e4-bc3c-9ce52082cb6b",
     "regions": {
      "83eddba8-c706-49a3-89bd-b510bc2c5008": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6bdd9f90-0795-44d8-a18d-c13301520398",
        "part": "whole"
       },
       "id": "83eddba8-c706-49a3-89bd-b510bc2c5008"
      }
     }
    },
    "90defec3-5b4d-470f-a68d-6a6fae9ebbec": {
     "id": "90defec3-5b4d-470f-a68d-6a6fae9ebbec",
     "prev": "8ee0867a-0485-4903-9e16-0b1a83bad931",
     "regions": {
      "fe8179b7-977e-48dc-be62-abf41f233a62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1d808ce4-f5b3-4c18-818d-04c00ee0d322",
        "part": "whole"
       },
       "id": "fe8179b7-977e-48dc-be62-abf41f233a62"
      }
     }
    },
    "92e07c6c-11e0-4795-bda1-abe9ddfd4d02": {
     "id": "92e07c6c-11e0-4795-bda1-abe9ddfd4d02",
     "prev": "0d01fc30-23a9-4dc7-a99a-309a23aa7a9b",
     "regions": {
      "fb6e6866-7208-41e1-b6ba-9450762a1241": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a3de943f-ece8-4dc5-8221-6611d4b59541",
        "part": "whole"
       },
       "id": "fb6e6866-7208-41e1-b6ba-9450762a1241"
      }
     }
    },
    "982cfab9-3c43-45c9-9733-6e3c27538085": {
     "id": "982cfab9-3c43-45c9-9733-6e3c27538085",
     "prev": "313101ba-a6ea-4985-b56a-7e5faeb4ef11",
     "regions": {
      "daed9365-6bd9-4bcb-a676-208bb12b4361": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4c8e853f-b925-4f45-82f2-36069d4f094b",
        "part": "whole"
       },
       "id": "daed9365-6bd9-4bcb-a676-208bb12b4361"
      }
     }
    },
    "9b077531-fbbf-4d74-8bd4-55550025f933": {
     "id": "9b077531-fbbf-4d74-8bd4-55550025f933",
     "prev": "1809123f-e0c6-4859-bc21-f3b5dc441a1b",
     "regions": {
      "3ec109fc-1ac8-4762-847e-5e6465f9cae8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "570ee83e-6db4-4a0e-b4ca-76c9665199d7",
        "part": "whole"
       },
       "id": "3ec109fc-1ac8-4762-847e-5e6465f9cae8"
      }
     }
    },
    "adb231cc-0652-40e4-bc3c-9ce52082cb6b": {
     "id": "adb231cc-0652-40e4-bc3c-9ce52082cb6b",
     "prev": "3ea64685-7182-46a8-b427-641f90be1dc9",
     "regions": {
      "da3e4bfe-8e1c-4315-bb8d-38def529ab4c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "464da1eb-7686-4f7d-a94f-fd4f4b3ae2e1",
        "part": "whole"
       },
       "id": "da3e4bfe-8e1c-4315-bb8d-38def529ab4c"
      }
     }
    },
    "c4d9cb29-6062-4d67-bacf-309165899909": {
     "id": "c4d9cb29-6062-4d67-bacf-309165899909",
     "prev": "f3ec050f-3d84-4833-87a4-3186460564fb",
     "regions": {
      "4429e8a1-ec45-4949-923e-5e7b08049d2d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8757fda7-751c-4dcd-80c7-fa4097aa62a3",
        "part": "whole"
       },
       "id": "4429e8a1-ec45-4949-923e-5e7b08049d2d"
      }
     }
    },
    "cbf62be1-d44f-43e8-a594-bba802494845": {
     "id": "cbf62be1-d44f-43e8-a594-bba802494845",
     "prev": "c4d9cb29-6062-4d67-bacf-309165899909",
     "regions": {
      "f304038b-504c-46f6-b03c-fc564190e993": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "14bef84c-7b2e-46b7-ac4f-18b9854d609e",
        "part": "whole"
       },
       "id": "f304038b-504c-46f6-b03c-fc564190e993"
      }
     }
    },
    "cef2e9fb-fb0c-4d2c-9264-fa08d3de7788": {
     "id": "cef2e9fb-fb0c-4d2c-9264-fa08d3de7788",
     "prev": "1315de08-3be9-4fc0-81e0-acc069ac5044",
     "regions": {
      "dc60dfcf-1616-4f16-82a0-cb0fade4e740": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "84a1b72e-5d2e-4343-a014-0fcfee9373e4",
        "part": "whole"
       },
       "id": "dc60dfcf-1616-4f16-82a0-cb0fade4e740"
      }
     }
    },
    "d6d405a0-dfde-4a37-a754-7db5e780e0af": {
     "id": "d6d405a0-dfde-4a37-a754-7db5e780e0af",
     "prev": "4f4a9683-905f-4595-9760-d56a847ae47c",
     "regions": {
      "cb634b1b-130b-4ddc-a273-f4d4b766feca": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b0f69589-df64-4288-985f-2cec5d01472f",
        "part": "whole"
       },
       "id": "cb634b1b-130b-4ddc-a273-f4d4b766feca"
      }
     }
    },
    "dcf0e1b4-2b46-4603-bb28-5f8e80ffd702": {
     "id": "dcf0e1b4-2b46-4603-bb28-5f8e80ffd702",
     "prev": "e67834a8-52ef-48d9-90b7-a291acb99e4b",
     "regions": {
      "0789a766-9218-417b-8534-24a6d4772988": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9d90f76f-96f5-4405-a1f5-4cf4eb5522fd",
        "part": "whole"
       },
       "id": "0789a766-9218-417b-8534-24a6d4772988"
      }
     }
    },
    "dd511c07-9814-43e5-b7c0-31151b3582e9": {
     "id": "dd511c07-9814-43e5-b7c0-31151b3582e9",
     "prev": "eab010b8-7397-4b90-835d-bf2e90bff56c",
     "regions": {
      "bb5d1405-461c-4569-86dd-b21c981872ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d7e940c4-4812-411a-94d3-7279c2de20cc",
        "part": "whole"
       },
       "id": "bb5d1405-461c-4569-86dd-b21c981872ed"
      }
     }
    },
    "e67834a8-52ef-48d9-90b7-a291acb99e4b": {
     "id": "e67834a8-52ef-48d9-90b7-a291acb99e4b",
     "prev": "cbf62be1-d44f-43e8-a594-bba802494845",
     "regions": {
      "9be8c217-3af4-451c-833e-fc9e910cb804": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ec118c50-e3fc-4dd2-9376-3e598f98167b",
        "part": "whole"
       },
       "id": "9be8c217-3af4-451c-833e-fc9e910cb804"
      }
     }
    },
    "eab010b8-7397-4b90-835d-bf2e90bff56c": {
     "id": "eab010b8-7397-4b90-835d-bf2e90bff56c",
     "prev": "7d5b2ad9-38cb-4ce1-8c24-7774740b8710",
     "regions": {
      "31363f19-2d8a-4bb1-9c82-5125bf553b08": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1b6580a7-ed0d-40c8-b2a7-a413193deae3",
        "part": "whole"
       },
       "id": "31363f19-2d8a-4bb1-9c82-5125bf553b08"
      }
     }
    },
    "f3ec050f-3d84-4833-87a4-3186460564fb": {
     "id": "f3ec050f-3d84-4833-87a4-3186460564fb",
     "prev": "1265d14e-4373-4961-b094-fdc0f41bd665",
     "regions": {
      "9dbd9105-2145-44ca-8fb6-f23ff8438a46": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7770a42f-6089-4f0a-85a9-025f72d3ad5a",
        "part": "whole"
       },
       "id": "9dbd9105-2145-44ca-8fb6-f23ff8438a46"
      }
     }
    },
    "f6cecdf6-a875-43e5-8544-93b4f9abb66d": {
     "id": "f6cecdf6-a875-43e5-8544-93b4f9abb66d",
     "prev": "90defec3-5b4d-470f-a68d-6a6fae9ebbec",
     "regions": {
      "8001bab8-0471-4f0a-9d92-c14df7dfea7d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "945892c8-340a-4bf8-bbcf-1aa29c546187",
        "part": "whole"
       },
       "id": "8001bab8-0471-4f0a-9d92-c14df7dfea7d"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
